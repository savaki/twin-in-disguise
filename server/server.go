// Copyright 2025 Matt Ho
//
// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at
//
//     http://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.

package server

import (
	"bytes"
	"context"
	"encoding/json"
	"fmt"
	"io"
	"log"
	"net/http"
	"sync"

	"github.com/google/generative-ai-go/genai"
	"github.com/savaki/twin-in-disguise/translator"
	"github.com/savaki/twin-in-disguise/types"
)

// Server handles Anthropic API requests and proxies them to Gemini
type Server struct {
	geminiClient        *genai.Client
	geminiHTTPClient    *translator.GeminiHTTPClient
	debug               bool
	thoughtSignatures   map[string]string // Maps tool_use ID to thought signature
	thoughtSignaturesMu sync.RWMutex
}

// New creates a new proxy server
func New(geminiClient *genai.Client) *Server {
	return &Server{
		geminiClient:      geminiClient,
		thoughtSignatures: make(map[string]string),
	}
}

// NewWithAPIKey creates a new proxy server with HTTP client support for thought signatures
func NewWithAPIKey(geminiClient *genai.Client, apiKey string) *Server {
	return &Server{
		geminiClient:      geminiClient,
		geminiHTTPClient:  translator.NewGeminiHTTPClient(apiKey),
		thoughtSignatures: make(map[string]string),
	}
}

// SetDebug enables or disables debug logging
func (s *Server) SetDebug(debug bool) {
	s.debug = debug
}

// HandleMessages handles POST /v1/messages requests
func (s *Server) HandleMessages(w http.ResponseWriter, r *http.Request) {
	ctx := r.Context()

	if s.debug {
		log.Printf("[DEBUG] API Request received:")
		log.Printf("[DEBUG]   Method: %s", r.Method)
		log.Printf("[DEBUG]   Path: %s", r.URL.Path)
		log.Printf("[DEBUG]   Remote: %s", r.RemoteAddr)
		log.Printf("[DEBUG]   Content-Length: %d", r.ContentLength)
	}

	// Parse Anthropic request
	// Read body
	body, err := io.ReadAll(r.Body)
	if err != nil {
		respondError(w, http.StatusBadRequest, fmt.Sprintf("Failed to read request body: %v", err))
		return
	}

	// Parse Anthropic request
	var anthropicReq types.AnthropicRequest
	if err := json.Unmarshal(body, &anthropicReq); err != nil {
		log.Printf("Failed to parse request: %v\nRequest body: %s", err, string(body))
		respondError(w, http.StatusBadRequest, fmt.Sprintf("Failed to parse request: %v", err))
		return
	}

	// Use the model from the request body directly
	geminiModelID := anthropicReq.Model

	if s.debug {
		log.Printf("[DEBUG]   Model: %s", geminiModelID)
	}

	log.Printf("Request: model=%s", geminiModelID)

	// Generate content
	anthropicResp, err := s.generateContent(ctx, geminiModelID, &anthropicReq)
	if err != nil {
		// Pretty print the request body for the log
		var prettyRequest bytes.Buffer
		if err := json.Indent(&prettyRequest, body, "", "  "); err != nil {
			prettyRequest.WriteString(string(body)) // Fallback to raw
		}

		log.Printf("Generation error: %v\nRequest body:\n%s", err, prettyRequest.String())
		respondError(w, http.StatusInternalServerError, fmt.Sprintf("Generation failed: %v", err))
		return
	}

	// Send response
	respondJSON(w, http.StatusOK, anthropicResp)
}

// injectThoughtSignatures injects cached thought signatures into tool_use blocks.
//
// Gemini requires thought signatures for multi-turn function calling conversations.
// This method restores thought signatures that were previously generated by Gemini
// when tool calls were made, allowing subsequent requests to reference those tools
// properly.
//
// How it works:
//  1. When Gemini generates a tool_use response, it includes a thought_signature
//  2. We cache this signature using the tool_use ID as the key
//  3. In subsequent requests, when the client sends back the same tool_use ID
//     (e.g., in tool_result blocks), we inject the cached signature back
//  4. This allows Gemini to maintain context about previous function calls
//
// The thought signature cache is stored in memory for the lifetime of the server.
//
// TODO: The thought signature cache is never garbage collected and will slowly
// leak memory over time. If this project gains traction, implement proper cache
// eviction (e.g., LRU cache with TTL or size limits).
func (s *Server) injectThoughtSignatures(req *types.AnthropicRequest) {
	s.thoughtSignaturesMu.RLock()
	defer s.thoughtSignaturesMu.RUnlock()

	for i := range req.Messages {
		for j := range req.Messages[i].Content {
			block := &req.Messages[i].Content[j]
			if block.Type == types.ContentTypeToolUse && block.ID != "" {
				if sig, ok := s.thoughtSignatures[block.ID]; ok {
					block.ThoughtSignature = sig
					if s.debug {
						log.Printf("[DEBUG] Injected thought signature for tool_use %s", block.ID)
					}
				}
			}
		}
	}
}

// cacheThoughtSignatures caches thought signatures from the response
func (s *Server) cacheThoughtSignatures(resp *types.AnthropicResponse) {
	s.thoughtSignaturesMu.Lock()
	defer s.thoughtSignaturesMu.Unlock()

	for _, block := range resp.Content {
		if block.Type == types.ContentTypeToolUse && block.ID != "" && block.ThoughtSignature != "" {
			s.thoughtSignatures[block.ID] = block.ThoughtSignature
			if s.debug {
				log.Printf("[DEBUG] Cached thought signature for tool_use %s", block.ID)
			}
		}
	}
}

func (s *Server) generateContent(ctx context.Context, modelID string, req *types.AnthropicRequest) (*types.AnthropicResponse, error) {
	// Check if we have tools in the request (which require thought signature support)
	hasTools := len(req.Tools) > 0

	// Inject cached thought signatures into the request
	s.injectThoughtSignatures(req)

	// Check if we have thought signatures in the messages (after injection)
	hasThoughtSignatures := false
	for _, msg := range req.Messages {
		for _, block := range msg.Content {
			if block.ThoughtSignature != "" {
				hasThoughtSignatures = true
				break
			}
		}
		if hasThoughtSignatures {
			break
		}
	}

	// Use HTTP client if we have tools or thought signatures, and the HTTP client is available
	// This is necessary because:
	// 1. Gemini requires thought signatures for function calling
	// 2. The genai SDK doesn't support thought signatures
	// 3. We need to preserve thought signatures across multi-turn conversations
	if (hasTools || hasThoughtSignatures) && s.geminiHTTPClient != nil {
		resp, err := s.generateContentWithHTTP(ctx, modelID, req)
		if err != nil {
			return nil, err
		}
		// Cache thought signatures from the response
		s.cacheThoughtSignatures(resp)
		return resp, nil
	}

	// Otherwise use the standard genai SDK
	resp, err := s.generateContentWithSDK(ctx, modelID, req)
	if err != nil {
		return nil, err
	}
	// Cache thought signatures from the response (if any)
	s.cacheThoughtSignatures(resp)
	return resp, nil
}

func (s *Server) generateContentWithHTTP(ctx context.Context, modelID string, req *types.AnthropicRequest) (*types.AnthropicResponse, error) {
	// Convert messages to custom Gemini contents (with thought signature support)
	contents, err := translator.ToCustomGeminiContents(req.Messages)
	if err != nil {
		return nil, fmt.Errorf("failed to convert messages: %w", err)
	}

	// Build request
	geminiReq := &translator.GenerateContentRequest{
		Contents: contents,
	}

	// Configure system instruction
	if req.System != nil {
		var systemPrompt string
		switch v := req.System.(type) {
		case string:
			systemPrompt = v
		case []interface{}:
			for _, item := range v {
				if m, ok := item.(map[string]interface{}); ok {
					if t, ok := m[types.SchemaFieldType].(string); ok && t == types.ContentTypeText {
						if text, ok := m["text"].(string); ok {
							systemPrompt += text + "\n"
						}
					}
				}
			}
		}

		if systemPrompt != "" {
			geminiReq.SystemInstruction = &types.GeminiContent{
				Parts: []types.GeminiPart{{Text: systemPrompt}},
			}
		}
	}

	// Configure generation parameters
	maxOutputTokens := int32(65536)
	geminiReq.GenerationConfig = &translator.GenerationConfig{
		MaxOutputTokens: &maxOutputTokens,
	}

	// Convert tools
	if len(req.Tools) > 0 {
		for _, tool := range req.Tools {
			geminiReq.Tools = append(geminiReq.Tools, translator.GeminiToolWrapper{
				FunctionDeclarations: []translator.FunctionDeclaration{{
					Name:        tool.Name,
					Description: tool.Description,
					Parameters:  translator.CleanSchemaForGemini(tool.InputSchema),
				}},
			})
		}
	}

	if s.debug {
		log.Printf("[DEBUG] Using HTTP client for thought signature support")
		log.Printf("[DEBUG]   Model: %s", modelID)
		log.Printf("[DEBUG]   Message count: %d", len(contents))
	}

	// Call Gemini API via HTTP
	resp, err := s.geminiHTTPClient.GenerateContent(ctx, modelID, geminiReq)
	if err != nil {
		return nil, fmt.Errorf("gemini API error: %w", err)
	}

	// Convert response
	anthropicResp, err := translator.ToAnthropicResponseFromCustom(resp, modelID)
	if err != nil {
		return nil, fmt.Errorf("failed to convert response: %w", err)
	}

	return anthropicResp, nil
}

func (s *Server) generateContentWithSDK(ctx context.Context, modelID string, req *types.AnthropicRequest) (*types.AnthropicResponse, error) {
	// Create Gemini model
	model := s.geminiClient.GenerativeModel(modelID)

	// Configure system instruction
	if req.System != nil {
		var systemPrompt string
		switch v := req.System.(type) {
		case string:
			systemPrompt = v
		case []interface{}:
			for _, item := range v {
				if m, ok := item.(map[string]interface{}); ok {
					if t, ok := m[types.SchemaFieldType].(string); ok && t == types.ContentTypeText {
						if text, ok := m["text"].(string); ok {
							systemPrompt += text + "\n"
						}
					}
				}
			}
		}

		if systemPrompt != "" {
			model.SystemInstruction = &genai.Content{
				Parts: []genai.Part{genai.Text(systemPrompt)},
			}
		}
	}

	// Configure generation parameters
	maxOutputTokens := int32(65536)
	model.GenerationConfig.MaxOutputTokens = &maxOutputTokens

	// Convert tools
	if len(req.Tools) > 0 {
		tools, err := translator.ToGeminiTools(req.Tools)
		if err != nil {
			return nil, fmt.Errorf("failed to convert tools: %w", err)
		}
		model.Tools = tools
	}

	// Convert messages to contents
	contents, err := translator.ToGeminiContents(req.Messages)
	if err != nil {
		return nil, fmt.Errorf("failed to convert messages: %w", err)
	}

	if s.debug {
		log.Printf("[DEBUG] Calling Gemini API:")
		log.Printf("[DEBUG]   Model: %s", modelID)
		log.Printf("[DEBUG]   Message count: %d", len(contents))
		log.Printf("[DEBUG]   System instruction: %v", req.System != nil)
		log.Printf("[DEBUG]   Tools: %d", len(req.Tools))
		log.Printf("[DEBUG]   Max tokens: %d", req.MaxTokens)
	}

	// Call Gemini API using chat session for multi-turn conversations
	var resp *genai.GenerateContentResponse
	if len(contents) == 1 {
		// Single turn - use GenerateContent directly
		if s.debug {
			log.Printf("[DEBUG] Using single-turn GenerateContent")
		}
		resp, err = model.GenerateContent(ctx, contents[0].Parts...)
	} else {
		// Multi-turn - use chat session
		if s.debug {
			log.Printf("[DEBUG] Using multi-turn chat session (history: %d messages)", len(contents)-1)
		}
		chat := model.StartChat()
		chat.History = contents[:len(contents)-1]
		resp, err = chat.SendMessage(ctx, contents[len(contents)-1].Parts...)
	}
	if err != nil {
		return nil, fmt.Errorf("gemini API error: %w", err)
	}

	if s.debug {
		log.Printf("[DEBUG] Gemini API response:")
		log.Printf("[DEBUG]   Candidates: %d", len(resp.Candidates))
		if resp.UsageMetadata != nil {
			log.Printf("[DEBUG]   Input tokens: %d", resp.UsageMetadata.PromptTokenCount)
			log.Printf("[DEBUG]   Output tokens: %d", resp.UsageMetadata.CandidatesTokenCount)
			log.Printf("[DEBUG]   Total tokens: %d", resp.UsageMetadata.TotalTokenCount)
		}
	}

	// Convert response
	anthropicResp, err := translator.ToAnthropicResponse(resp, modelID)
	if err != nil {
		return nil, fmt.Errorf("failed to convert response: %w", err)
	}

	return anthropicResp, nil
}

func respondJSON(w http.ResponseWriter, status int, data interface{}) {
	w.Header().Set("Content-Type", "application/json")
	w.WriteHeader(status)
	if err := json.NewEncoder(w).Encode(data); err != nil {
		log.Printf("Failed to encode response: %v", err)
	}
}

func respondError(w http.ResponseWriter, status int, message string) {
	response := map[string]string{types.ResponseFieldError: message}

	// Log the error response
	if status >= 400 {
		log.Printf("Responding with error %d: %s", status, message)
	}

	respondJSON(w, status, response)
}
